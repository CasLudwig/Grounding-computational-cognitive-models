---
title: 'Local Sampling and Comparison'
link-citations: yes
output:
  html_document: default
  html_notebook: default
  pdf_document: default
# bibliography: X.bib
---
```{r set_up, echo=FALSE,include=FALSE}
rm(list=ls())
# Load libraries
library(tidyverse)
library(viridis)
library(latex2exp)
library(gganimate)

# Set working directory
setwd("/Users/casimirludwig/OneDrive - University of Bristol/Data/localSamplingComparison")

# Load relevant functions
source("objectiveFunctions.R")
source("samplingFunctions.R")
```

In Ludwig, Stuchl√Ω, and Malhotra (in revision), we put forward a Local Sampling and Comparison algorithm for navigating a multi-dimensional objective function. Details of the algorithm can be found in the manuscript. In the manuscript, we assess the performance of the algorithm for optimising and satisficing agents, for a range of values of noise in the objective estimates and window sizes.

Here we explore the behaviour of this algorithm in more detail. Specifically, we explore the following factors.

- **Time horizon**. Our hypothesis is that the performance advantage of the satisficing agent depends on the amount of time available for exploring the state space. With a longer time horizon, the optimising agent will do better.
- **The granularity of objective knowledge**. So far we have assumed that the agent gets noisy, but metric information about the objective. This metric information is particularly important in governing the jump probability (and, indirectly, the stopping rule). We explore performance for an agent who only gets coarse knowledge about the objective (e.g. in 5 bins ranging from bad to very good), and uses this to guide their jump (and stopping) decisions.
- **Prior knowledge of the objective peak**. So far we have assumed that the agent has some (metric) knowledge of what the maximum attainable value of the objective function is or, at least, what sort of value is good enough, which also implies some sense of the scale over which the objective can vary. In many situations, the agent will only get a sense of the scale of the objective as they are exploring it. One way to address this is to implement a stopping criterion that depends on the relative improvements across jumps and the number of steps taken.
- **The nature of the jump distribution and its scale**. We will compare a bivariate Gaussian distribution with a uniform distribution, each with two different scales.
- **The nature of the jump probability function**. In the initial formulation, the probability of a jump depends inversely and linearly on the objective value relative to the peak (clipped to the range between 0-1). We will explore some other forms: sigmoidal (straightforward dependence on value, so no need to compare with a threshold value); step+$\epsilon$-greedy (always jump when the value falls below a certain threshold, otherwise jump with a fixed probability $\epsilon$).
- **The nature of the objective function itself**. We will vary the smoothness of this function (although to some extent our noise manipulation is effectively such a manipulation) and the number of peaks (unimodal vs multimodal).
- Finally, we need to compare the algorithm against simpler versions. For example, we could abolish the comparison with the previous location and simply let the jump probability vary inversely with the estimated objective value.

# Optimising and satisficing agents under different time horizons

```{r timeHorizonSimulation, echo=FALSE, include=FALSE}

# Run the simulation

source("timeHorizonSimulation.R")

# Specify the variables we want to combine and the number of simulations to run for each combination.
simN <- 1000 # For each combination of variables, how many simulations to run?
timeHorizon <- c(250, 500, 1000, 2000) # Time horizon
noise <- c(10, 50) # Just pick a relatively low and very high noise level
trialWindow <- c(1, 5, 10, 25, 50) 
stopThreshold <- c(100, 75) # Stopping criterion for the search (correspond to maximising and satisficing, respectively).

suppressWarnings(
  horizonResults <- timeHorizonSimulation(simulationN = simN, 
                      totalN = timeHorizon, 
                      sigma0 = noise,
                      windowSize = trialWindow,
                      satisficeThreshold = stopThreshold)
)
```

First, show the 2D objective function.

```{r echo=FALSE, include=TRUE}

# Show the 2D objective
suppressWarnings(print(horizonResults$pObj))
```
We characterise performance of the algorithm with three measures, averaged over the 1000 simulation runs: the gain per trial, the number of unique locations visited, the spatial spread of the trajectory (weighted standard deviation of the distances relative to the weighted average location). Below, we show each of these measures in turn. In these figures, the time horizon is varied in the columns and the noise level in the rows. The performance measures are shown as a function of window size.

```{r echo=FALSE, include=TRUE}

# Output the mean gain
suppressWarnings(print(horizonResults$pGain))
```
Note that the advantage of the satisficing agent is maintained across a wide range of conditions and, importantly, time horizons. We expected the maximising agent to gain an advantage for longer time horizons. After all, the cost of exploration might then be outweighed by being able to spend more time in a better spot. Of course this result will depend on the minimum satisfactory threshold adopted, the nature of the objective function and so forth.

We hypothesised that the satisficing agent performs better because they explore less. The following two figures support this hypothesis, at least for relatively low noise levels.

```{r echo=FALSE, include=TRUE}

# Output the mean number of unique locations
suppressWarnings(print(horizonResults$pLocs))
```
Here the number of unique locations visited is smaller for the satisficing agent than for the maximising agent. At high levels of noise (when it is more difficult to find a stable point on the objective function), both agents seem to visit a similar number of locations.

```{r echo=FALSE, include=TRUE}

# Output the mean spread
suppressWarnings(print(horizonResults$pSpread))
```

The spread of the visited locations is also smaller for the satisficing agent, again at least at relatively low noise values (and longer time horizons and smaller window sizes). 

# Granularity of objective knowledge

In the next set of simulations we assume that the agent can only obtain very coarse knowledge of the objective in $M$ categories (e.g. "very bad", "bad", "ok", "good", "very good"). In this set of simulations, we assume $M = 5$ with linearly spaced category boundaries over the range of the objective function, i.e. $b = \lbrace 0.2, 0.4, 0.6, 0.8 \rbrace$ (normalised units, specified relative to the peak of the objective function). The jump probability decreases linearly across these five bins, i.e. $\pi(c) = \lbrace 1, 0.75, 0.5, 0.25, 0 \rbrace$, for category $c = 1, \ldots, 5$. The "stay" or "return" decision is now driven by whether the categorical objective value is better or equal/worse than at the previous location.

The agent samples an objective location and forms a noisy estimate over the course of $W$ trials, exactly as in previous simulations. The agent does not have access to this estimate on the underlying objective scale. Rather, they only have access to a coarse, discrete representation of the objective estimate, determined by the mapping described above. Therefore, although having a greater window $W$ will still reduce the uncertainty in the objective estimate, this reduction in uncertainty only manifests itself in more accurate category assignment. The benefit of reducing uncertainty will therefore depend on the granularity of objective knowledge: of course, in the extreme case of $M = 100$, the benefit will similar to the cases above where the agent has direct access to underlying objective scale. One way of viewing the discrete transformation is as a form of additional noise. Indeed, it is likely that this form of noise will swamp at least the low - medium levels of noise in the underlying objective estimates. 

The agent stops exploring as soon as the objective value is categorised as "very good", i.e. $c = 5$, which under the current setup happens as soon as the estimated objective value is greater than 80% of the maximum. Therefore, in this set up the agent is satisficing rather than maximising. However, the extent to which the agent satisfices or optimises can be manipulated by changing category boundaries and/or the mapping between the categories and the jump probability (e.g. if the boundaries were such that $c = 5$ for $v /geq 100$, that would be equivalent to the maximising agent from the previous section).

In the following simulations, we will compare the agent with coarse, categorical knowledge of the objective with a satisficing agent who has access to the underlying metric scale. To enable a fair comparison between the two agents, we set $\gamma = 80$. As before, we will vary the window size and the uncertainty around the objective. 

```{r categoricalObjectiveSimulation, echo=FALSE, include=FALSE}

# Run the simulation
rm(list=ls())
source("objectiveFunctions.R")
source("samplingFunctions.R")
source("categoricalObjectiveSimulation.R")

# Specify the variables we want to combine and the number of simulations to run for each combination.
simN <- 1000 # For each combination of variables, how many simulations to run?
timeHorizon <- c(250, 500, 1000, 2000) # Time horizon
noise <- c(10, 50) # Just pick a relatively low and very high noise level
trialWindow <- c(1, 5, 10, 25, 50) 
objRep <- c("metric", "categorical")

suppressWarnings(
  catObjResults <- categoricalObjectiveSimulation(simulationN = simN, 
                      totalN = timeHorizon, 
                      sigma0 = noise,
                      windowSize = trialWindow,
                      objRepresentation = objRep)
)
```

As before, we show the averaged gain per trial, number of unique locations visited and spatial spread, as a function of window size (time horizon varied in columns; two noise levels in the rows). 

```{r echo=FALSE, include=TRUE}

# Output the mean gain
suppressWarnings(print(catObjResults$pGain))
```

```{r echo=FALSE, include=TRUE}

# Output the mean number of unique locations
suppressWarnings(print(catObjResults$pLocs))
```

```{r echo=FALSE, include=TRUE}

# Output the mean spread
suppressWarnings(print(catObjResults$pSpread))
```


